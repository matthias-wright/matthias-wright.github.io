<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" type="text/css" href="css/bootstrap.css">
    <link rel="stylesheet" type="text/css" href="css/main.css">
    <script type="text/javascript" src="js/jquery-3.3.1.js"></script>
    <script>
        $(function(){
            $("#header").load("header.html");
        });
    </script>
    <title>Thesis Projects</title>
</head>
<body>
<div class="wrappper">
    <div id="header"></div>
    <div class="content">
        <span class="heading1">Thesis Projects</span>
        <hr>
        <div class="row">
            <div class="col-sm-12">
                <span class="heading2">Neural Style Transfer with Deep Learning (Working Title)</span><br/>
                <span class="heading3">Master's thesis (May 2019 - Present)</span><br/>
                <span class="font1">
                    Similar to the CycleGAN architecture, the main purpose of this model is to define a mapping between two image domains X and Y without
                    the use of paired training data. A key difference is that the mapping does not directly translate between the two image domains.
                    Instead, two Autoencoders are incorporated into the model. Their aim is to transform the images from each domain into a low-dimensional representation.
                    The mapping then occurs between the two low-dimensional spaces of the two domains. Both autoencoders are pretrained and fixed during the training procedure.
                    The following figure shows the architecture. </br>

                    <div style="text-align: center;"><img src="img/architecture.png" width="400" alt="Architecture"></div>

                    The goal of the transformation network (generator) G<sub>V</sub> is to take a low-dimensional representation v of an image in X and translate
                    it to a low-dimensional representation u&#x0302;. Putting u&#x0302; through the decoder D<sub>Y</sub> should yield an image y&#x0302; that is
                    visually indistinguishable from images in Y. The transformation network D<sub>U</sub> has the same goal for the opposite direction. </br>
                    The discriminator C<sub>Y</sub> aims to distinguish between images from y from Y and generated images y&#x0302;. The discriminator C<sub>X</sub> works analogously. </br>
                    In addition to this adversarial training formulation, a <i>cycle consistency</i> constraint (Zhu et al., 2017) [1] is imposed on the architecture.
                    This constraint encourages images that are translated to the opposite domain and then translated back to look approximately like the original image. </br>
                    All neural networks were implemented as convolutional neural networks. The experiments were implemented in Python together with PyTorch.
                    </span>
                    <span class="font1">
                    [1] Zhu J-Y, Park T, Isola P, and Efros AA. 2017.
                    Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.
                    IEEE International Conference on Computer Vision (ICCV), pp.2242-2251.
                    </span>
            </div>
            <div class="col-sm-12"></div>
            <div class="col-sm-12">
                <span class="heading2">Detecting Abnormalities in Clinical Data with Generative Adversarial Networks</span><br/>
                <span class="heading3">Bachelor's thesis (April 2018 - August 2018)</span><br/>
                <a href="files/ba_thesis.pdf" target="_blank">Full version [PDF]</a>&nbsp;
                <a href="files/ba_thesis_short.pdf" target="_blank">Short version [PDF]</a>&nbsp;
                <a href="https://github.com/matthias-wright/ba_thesis" target="_blank">Code [GitHub]</a><br/>
                <span class="font1">Detection of medical conditions or abnormalities in clinical
                    data has already been achieved by employing discriminative models, especially
                    neural networks. For example, neural networks have been trained as binary
                    classifiers to learn a decision boundary that discriminates between normal
                    and abnormal ECG recordings. The training process requires normal as well as
                    abnormal ECG recordings. However, public ECG databases generally consist of
                    many normal recordings and a few abnormal recordings.
                    This circumstance served as the main incentive for my bachelor's thesis project, which was to train a
                    generative adversarial network with only normal data and to then examine whether it would be capable of
                    discriminating between normal and abnormal data. <br/>
                    During my research phase, I conducted two separate experiments. In one experiment, I examined
                    whether generative adversarial networks are capable of detecting cardiac arrhythmia in ECG recordings.
                    For this, I trained a generative adversarial network with normal ECG recordings. Afterwards
                    I fed two separate test sets into the trained discriminator, one containing normal ECG
                    recordings and the other containing arrhythmic ECG recordings. The hypothesis was that,
                    because the discriminator was trained only on normal recordings, the loss would be larger
                    for the test set containing arrhythmic recordings. For the other experiment, I repeated the
                    procedure in order to examine whether generative adversarial networks, when given the blood gas
                    measurements of a patient, can decide whether or not that patient has respiratory failure.
                    </span>
            </div>
        </div>
    </div>
</div>
<footer class="footer">
    <span id="footer-text">&copy; Matthias Wright, 2019</span>
</footer>
</body>
</html>